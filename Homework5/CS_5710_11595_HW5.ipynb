{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "987BTGHdThuP",
        "outputId": "608d75d7-03dc-4a16-ae73-285e1a5f9d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Testing Scaled Dot-Product Attention\n",
            "============================================================\n",
            "\n",
            "--- Example 1: Single Sequence ---\n",
            "Input shapes:\n",
            "  Q: (1, 4, 8)\n",
            "  K: (1, 4, 8)\n",
            "  V: (1, 4, 8)\n",
            "\n",
            "Output shapes:\n",
            "  Attention weights: (1, 4, 4)\n",
            "  Context vector: (1, 4, 8)\n",
            "\n",
            "Attention weights (first sequence):\n",
            "[[0.08431243 0.25513027 0.51521078 0.14534652]\n",
            " [0.64059204 0.1332861  0.01664257 0.2094793 ]\n",
            " [0.47006414 0.08789379 0.11121405 0.33082801]\n",
            " [0.17794451 0.49185018 0.20052305 0.12968226]]\n",
            "\n",
            "Sum of attention weights per row (should be ~1.0):\n",
            "[1. 1. 1. 1.]\n",
            "\n",
            "============================================================\n",
            "--- Example 2: Batch of Sequences ---\n",
            "Input shapes:\n",
            "  Q: (3, 5, 16)\n",
            "  K: (3, 5, 16)\n",
            "  V: (3, 5, 16)\n",
            "\n",
            "Output shapes:\n",
            "  Attention weights: (3, 5, 5)\n",
            "  Context vector: (3, 5, 16)\n",
            "\n",
            "============================================================\n",
            "--- Example 3: Cross-Attention (Q and K have different lengths) ---\n",
            "Input shapes:\n",
            "  Q: (2, 3, 8) (queries from decoder)\n",
            "  K: (2, 5, 8) (keys from encoder)\n",
            "  V: (2, 5, 8) (values from encoder)\n",
            "\n",
            "Output shapes:\n",
            "  Attention weights: (2, 3, 5)\n",
            "  Context vector: (2, 3, 8)\n",
            "\n",
            "============================================================\n",
            "All tests completed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Question 1: Compute Scaled Dot-Product Attention (Python)\n",
        "\n",
        "This implementation computes scaled dot-product attention given Q, K, and V matrices.\n",
        "Formula: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        Q: Query matrix of shape (batch_size, seq_len_q, d_k)\n",
        "        K: Key matrix of shape (batch_size, seq_len_k, d_k)\n",
        "        V: Value matrix of shape (batch_size, seq_len_v, d_v)\n",
        "\n",
        "    Returns:\n",
        "        context: Context vector of shape (batch_size, seq_len_q, d_v)\n",
        "        attention_weights: Attention weights of shape (batch_size, seq_len_q, seq_len_k)\n",
        "    \"\"\"\n",
        "    # Step 1: Get the dimension d_k from the key matrix\n",
        "    d_k = K.shape[-1]\n",
        "\n",
        "    # Step 2: Compute attention scores: QK^T\n",
        "    # Using matrix multiplication (@ operator or np.matmul)\n",
        "    scores = np.matmul(Q, K.transpose(0, 2, 1))  # Shape: (batch_size, seq_len_q, seq_len_k)\n",
        "\n",
        "    # Step 3: Scale by sqrt(d_k)\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "\n",
        "    # Step 4: Apply softmax to get attention weights\n",
        "    # Softmax is applied along the last dimension (over keys)\n",
        "    attention_weights = softmax(scaled_scores)\n",
        "\n",
        "    # Step 5: Compute context vector by weighted sum of values\n",
        "    context = np.matmul(attention_weights, V)  # Shape: (batch_size, seq_len_q, d_v)\n",
        "\n",
        "    return attention_weights, context\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute softmax along the last dimension.\n",
        "    Uses numerical stability trick: subtract max before exp.\n",
        "\n",
        "    Args:\n",
        "        x: Input array of any shape\n",
        "\n",
        "    Returns:\n",
        "        Softmax probabilities with same shape as input\n",
        "    \"\"\"\n",
        "    # Subtract max for numerical stability\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "\n",
        "# ============= EXAMPLE USAGE AND TESTING =============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"Testing Scaled Dot-Product Attention\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Example 1: Single sequence\n",
        "    print(\"\\n--- Example 1: Single Sequence ---\")\n",
        "    batch_size = 1\n",
        "    seq_len = 4\n",
        "    d_k = 8  # dimension of keys/queries\n",
        "    d_v = 8  # dimension of values\n",
        "\n",
        "    Q = np.random.randn(batch_size, seq_len, d_k)\n",
        "    K = np.random.randn(batch_size, seq_len, d_k)\n",
        "    V = np.random.randn(batch_size, seq_len, d_v)\n",
        "\n",
        "    attention_weights, context = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "    print(f\"Input shapes:\")\n",
        "    print(f\"  Q: {Q.shape}\")\n",
        "    print(f\"  K: {K.shape}\")\n",
        "    print(f\"  V: {V.shape}\")\n",
        "    print(f\"\\nOutput shapes:\")\n",
        "    print(f\"  Attention weights: {attention_weights.shape}\")\n",
        "    print(f\"  Context vector: {context.shape}\")\n",
        "    print(f\"\\nAttention weights (first sequence):\")\n",
        "    print(attention_weights[0])\n",
        "    print(f\"\\nSum of attention weights per row (should be ~1.0):\")\n",
        "    print(np.sum(attention_weights[0], axis=-1))\n",
        "\n",
        "    # Example 2: Batch of sequences\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"--- Example 2: Batch of Sequences ---\")\n",
        "    batch_size = 3\n",
        "    seq_len = 5\n",
        "    d_k = 16\n",
        "    d_v = 16\n",
        "\n",
        "    Q = np.random.randn(batch_size, seq_len, d_k)\n",
        "    K = np.random.randn(batch_size, seq_len, d_k)\n",
        "    V = np.random.randn(batch_size, seq_len, d_v)\n",
        "\n",
        "    attention_weights, context = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "    print(f\"Input shapes:\")\n",
        "    print(f\"  Q: {Q.shape}\")\n",
        "    print(f\"  K: {K.shape}\")\n",
        "    print(f\"  V: {V.shape}\")\n",
        "    print(f\"\\nOutput shapes:\")\n",
        "    print(f\"  Attention weights: {attention_weights.shape}\")\n",
        "    print(f\"  Context vector: {context.shape}\")\n",
        "\n",
        "    # Example 3: Cross-attention (different sequence lengths)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"--- Example 3: Cross-Attention (Q and K have different lengths) ---\")\n",
        "    batch_size = 2\n",
        "    seq_len_q = 3  # Query sequence length\n",
        "    seq_len_k = 5  # Key/Value sequence length\n",
        "    d_k = 8\n",
        "    d_v = 8\n",
        "\n",
        "    Q = np.random.randn(batch_size, seq_len_q, d_k)\n",
        "    K = np.random.randn(batch_size, seq_len_k, d_k)\n",
        "    V = np.random.randn(batch_size, seq_len_k, d_v)\n",
        "\n",
        "    attention_weights, context = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "    print(f\"Input shapes:\")\n",
        "    print(f\"  Q: {Q.shape} (queries from decoder)\")\n",
        "    print(f\"  K: {K.shape} (keys from encoder)\")\n",
        "    print(f\"  V: {V.shape} (values from encoder)\")\n",
        "    print(f\"\\nOutput shapes:\")\n",
        "    print(f\"  Attention weights: {attention_weights.shape}\")\n",
        "    print(f\"  Context vector: {context.shape}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"All tests completed successfully!\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 2: Implement Simple Transformer Encoder Block (PyTorch)\n",
        "\n",
        "This implementation includes:\n",
        "- Multi-head self-attention layer\n",
        "- Feed-forward network (2 linear layers with ReLU)\n",
        "- Add & Norm layers (residual connections + layer normalization)\n",
        "\n",
        "Parameters: d_model = 128, num_heads = 8\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head self-attention mechanism.\n",
        "    Splits input into multiple heads, applies scaled dot-product attention,\n",
        "    then concatenates and projects back.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # Dimension per head\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, d_k).\n",
        "\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, num_heads, seq_len, d_k)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        # Reshape and transpose\n",
        "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
        "        return x.transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"\n",
        "        Combine heads back together.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, num_heads, seq_len, d_k)\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size, num_heads, seq_len, d_k = x.size()\n",
        "        x = x.transpose(1, 2).contiguous()  # (batch_size, seq_len, num_heads, d_k)\n",
        "        return x.view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Compute scaled dot-product attention.\n",
        "\n",
        "        Args:\n",
        "            Q, K, V: Tensors of shape (batch_size, num_heads, seq_len, d_k)\n",
        "            mask: Optional mask tensor\n",
        "\n",
        "        Returns:\n",
        "            context: Tensor of shape (batch_size, num_heads, seq_len, d_k)\n",
        "            attention_weights: Tensor of shape (batch_size, num_heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Compute context\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass of multi-head attention.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional attention mask\n",
        "\n",
        "        Returns:\n",
        "            output: Tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Linear projections\n",
        "        Q = self.W_q(x)  # (batch_size, seq_len, d_model)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        Q = self.split_heads(Q)  # (batch_size, num_heads, seq_len, d_k)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "\n",
        "        # Apply attention\n",
        "        context, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads\n",
        "        context = self.combine_heads(context)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise feed-forward network.\n",
        "    Two linear transformations with ReLU activation in between.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: Linear -> ReLU -> Linear\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        return self.linear2(F.relu(self.linear1(x)))\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single transformer encoder block with:\n",
        "    - Multi-head self-attention\n",
        "    - Feed-forward network\n",
        "    - Residual connections and layer normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = MultiHeadSelfAttention(d_model, num_heads)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass with residual connections and layer normalization.\n",
        "\n",
        "        Architecture:\n",
        "        1. x -> Multi-head Attention -> Add & Norm\n",
        "        2. x -> Feed-Forward -> Add & Norm\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional attention mask\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Multi-head attention with residual connection and layer norm\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))  # Add & Norm\n",
        "\n",
        "        # Feed-forward network with residual connection and layer norm\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))  # Add & Norm\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============= TESTING AND VERIFICATION =============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"Testing Transformer Encoder Block\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Initialize dimensions as specified\n",
        "    d_model = 128\n",
        "    num_heads = 8\n",
        "    d_ff = 512  # Typical: 4 * d_model\n",
        "    dropout = 0.1\n",
        "\n",
        "    print(f\"\\nModel Configuration:\")\n",
        "    print(f\"  d_model: {d_model}\")\n",
        "    print(f\"  num_heads: {num_heads}\")\n",
        "    print(f\"  d_ff: {d_ff}\")\n",
        "    print(f\"  d_k (per head): {d_model // num_heads}\")\n",
        "\n",
        "    # Create the encoder block\n",
        "    encoder_block = TransformerEncoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "\n",
        "    # Print model architecture\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Model Architecture:\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(encoder_block)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in encoder_block.parameters())\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Total Parameters: {total_params:,}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Test with batch of 32 sentences, each with 10 tokens\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Verification Test: Batch of 32 sentences, 10 tokens each\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    batch_size = 32\n",
        "    seq_len = 10\n",
        "\n",
        "    # Create random input (simulating token embeddings)\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    print(f\"\\nInput shape: {x.shape}\")\n",
        "    print(f\"  Batch size: {batch_size}\")\n",
        "    print(f\"  Sequence length: {seq_len}\")\n",
        "    print(f\"  Model dimension: {d_model}\")\n",
        "\n",
        "    # Set model to evaluation mode for testing\n",
        "    encoder_block.eval()\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output = encoder_block(x)\n",
        "\n",
        "    print(f\"\\nOutput shape: {output.shape}\")\n",
        "\n",
        "    # Verify shape\n",
        "    expected_shape = (batch_size, seq_len, d_model)\n",
        "    assert output.shape == expected_shape, f\"Shape mismatch! Expected {expected_shape}, got {output.shape}\"\n",
        "\n",
        "    print(f\"\\n✓ Shape verification passed!\")\n",
        "    print(f\"  Expected: {expected_shape}\")\n",
        "    print(f\"  Got: {output.shape}\")\n",
        "\n",
        "    # Additional statistics\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Output Statistics:\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Mean: {output.mean().item():.6f}\")\n",
        "    print(f\"  Std: {output.std().item():.6f}\")\n",
        "    print(f\"  Min: {output.min().item():.6f}\")\n",
        "    print(f\"  Max: {output.max().item():.6f}\")\n",
        "\n",
        "    # Test individual components\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Testing Individual Components:\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Test Multi-Head Attention\n",
        "    attention = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    with torch.no_grad():\n",
        "        attn_output = attention(x)\n",
        "    print(f\"\\n✓ Multi-Head Attention output shape: {attn_output.shape}\")\n",
        "\n",
        "    # Test Feed-Forward Network\n",
        "    ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "    with torch.no_grad():\n",
        "        ffn_output = ffn(x)\n",
        "    print(f\"✓ Feed-Forward Network output shape: {ffn_output.shape}\")\n",
        "\n",
        "    # Test with different batch sizes and sequence lengths\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Testing with Various Input Sizes:\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    test_cases = [\n",
        "        (1, 5),      # Single sentence, 5 tokens\n",
        "        (16, 20),    # 16 sentences, 20 tokens\n",
        "        (64, 50),    # 64 sentences, 50 tokens\n",
        "    ]\n",
        "\n",
        "    for batch, seq in test_cases:\n",
        "        test_input = torch.randn(batch, seq, d_model)\n",
        "        with torch.no_grad():\n",
        "            test_output = encoder_block(test_input)\n",
        "        print(f\"  Input: ({batch}, {seq}, {d_model}) -> Output: {test_output.shape} ✓\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"All tests completed successfully!\")\n",
        "    print(f\"{'='*70}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8DFolddUO3H",
        "outputId": "b6ac3d1e-6b4d-4840-eba6-1148dd2b4ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Testing Transformer Encoder Block\n",
            "======================================================================\n",
            "\n",
            "Model Configuration:\n",
            "  d_model: 128\n",
            "  num_heads: 8\n",
            "  d_ff: 512\n",
            "  d_k (per head): 16\n",
            "\n",
            "======================================================================\n",
            "Model Architecture:\n",
            "======================================================================\n",
            "TransformerEncoderBlock(\n",
            "  (attention): MultiHeadSelfAttention(\n",
            "    (W_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (W_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (W_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (W_o): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (ffn): FeedForwardNetwork(\n",
            "    (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
            "    (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "\n",
            "======================================================================\n",
            "Total Parameters: 198,272\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Verification Test: Batch of 32 sentences, 10 tokens each\n",
            "======================================================================\n",
            "\n",
            "Input shape: torch.Size([32, 10, 128])\n",
            "  Batch size: 32\n",
            "  Sequence length: 10\n",
            "  Model dimension: 128\n",
            "\n",
            "Output shape: torch.Size([32, 10, 128])\n",
            "\n",
            "✓ Shape verification passed!\n",
            "  Expected: (32, 10, 128)\n",
            "  Got: torch.Size([32, 10, 128])\n",
            "\n",
            "======================================================================\n",
            "Output Statistics:\n",
            "======================================================================\n",
            "  Mean: -0.000000\n",
            "  Std: 1.000008\n",
            "  Min: -3.913818\n",
            "  Max: 4.123004\n",
            "\n",
            "======================================================================\n",
            "Testing Individual Components:\n",
            "======================================================================\n",
            "\n",
            "✓ Multi-Head Attention output shape: torch.Size([32, 10, 128])\n",
            "✓ Feed-Forward Network output shape: torch.Size([32, 10, 128])\n",
            "\n",
            "======================================================================\n",
            "Testing with Various Input Sizes:\n",
            "======================================================================\n",
            "  Input: (1, 5, 128) -> Output: torch.Size([1, 5, 128]) ✓\n",
            "  Input: (16, 20, 128) -> Output: torch.Size([16, 20, 128]) ✓\n",
            "  Input: (64, 50, 128) -> Output: torch.Size([64, 50, 128]) ✓\n",
            "\n",
            "======================================================================\n",
            "All tests completed successfully!\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}